{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries \n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train-Test Split \n",
    "2. Data Preprocessing (change to black and white, getting the facial shape etc) \n",
    "3. Data Augmentation etc .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do: \n",
    "\n",
    "1. Add in F1 Metrics \n",
    "2. Figure out how to add tensorboard to get training diagnostics \n",
    "3. Progressive is implemented but need to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 trained from scratch\n",
    "class Resnet50_scratch_dag(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Resnet50_scratch_dag, self).__init__()\n",
    "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
    "                     'std': [1, 1, 1],\n",
    "                     'imageSize': [224, 224, 3]}\n",
    "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
    "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
    "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_3x3_relu = nn.ReLU()\n",
    "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_relu = nn.ReLU()\n",
    "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_3x3_relu = nn.ReLU()\n",
    "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_relu = nn.ReLU()\n",
    "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_3x3_relu = nn.ReLU()\n",
    "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_relu = nn.ReLU()\n",
    "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_3x3_relu = nn.ReLU()\n",
    "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_relu = nn.ReLU()\n",
    "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_3x3_relu = nn.ReLU()\n",
    "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_relu = nn.ReLU()\n",
    "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_3x3_relu = nn.ReLU()\n",
    "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_relu = nn.ReLU()\n",
    "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_3x3_relu = nn.ReLU()\n",
    "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_relu = nn.ReLU()\n",
    "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_3x3_relu = nn.ReLU()\n",
    "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_relu = nn.ReLU()\n",
    "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_3x3_relu = nn.ReLU()\n",
    "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_relu = nn.ReLU()\n",
    "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_3x3_relu = nn.ReLU()\n",
    "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_relu = nn.ReLU()\n",
    "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_3x3_relu = nn.ReLU()\n",
    "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_relu = nn.ReLU()\n",
    "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_3x3_relu = nn.ReLU()\n",
    "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_relu = nn.ReLU()\n",
    "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_3x3_relu = nn.ReLU()\n",
    "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_relu = nn.ReLU()\n",
    "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_3x3_relu = nn.ReLU()\n",
    "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_relu = nn.ReLU()\n",
    "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_3x3_relu = nn.ReLU()\n",
    "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_relu = nn.ReLU()\n",
    "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_3x3_relu = nn.ReLU()\n",
    "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_relu = nn.ReLU()\n",
    "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
    "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
    "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
    "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
    "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
    "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
    "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
    "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
    "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
    "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
    "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
    "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
    "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
    "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
    "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
    "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
    "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
    "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
    "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
    "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
    "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
    "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
    "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
    "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
    "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
    "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
    "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
    "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
    "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
    "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
    "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
    "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
    "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
    "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
    "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
    "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
    "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
    "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
    "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
    "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
    "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
    "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
    "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
    "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
    "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
    "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
    "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
    "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
    "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
    "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
    "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
    "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
    "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
    "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
    "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
    "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
    "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
    "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
    "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
    "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
    "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
    "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
    "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
    "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
    "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
    "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
    "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
    "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
    "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
    "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
    "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
    "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
    "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
    "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
    "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
    "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
    "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
    "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
    "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
    "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
    "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
    "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
    "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
    "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
    "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
    "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
    "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
    "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
    "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
    "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
    "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
    "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
    "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
    "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
    "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
    "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
    "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
    "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
    "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
    "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
    "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
    "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
    "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
    "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
    "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
    "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
    "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
    "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
    "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
    "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
    "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
    "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
    "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
    "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
    "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
    "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
    "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
    "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
    "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
    "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
    "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
    "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
    "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
    "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
    "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
    "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
    "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
    "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
    "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
    "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
    "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
    "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
    "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
    "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
    "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
    "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
    "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
    "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
    "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
    "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
    "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
    "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
    "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
    "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
    "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
    "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
    "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
    "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
    "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
    "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
    "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
    "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
    "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
    "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
    "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
    "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
    "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
    "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
    "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
    "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
    "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
    "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
    "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
    "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
    "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
    "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
    "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
    "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
    "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
    "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
    "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
    "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
    "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
    "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
    "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
    "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
    "        return classifier, pool5_7x7_s1\n",
    "\n",
    "def resnet50_scratch_dag(weights_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    load imported model instance\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): If set, loads model weights from the given path\n",
    "    \"\"\"\n",
    "    model = Resnet50_scratch_dag()\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialExpressionModel:\n",
    "    \n",
    "    def __init__(self, model_name, num_classes,dataloaders,dataset_sizes,lr, momentum,\n",
    "                 feature_extract=False, use_pretrained=True):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_extract = feature_extract\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.learning_rate = lr\n",
    "        self.momentum = momentum \n",
    "        self.model_ft = None\n",
    "        self.input_size = None\n",
    "        self.dataloaders = dataloaders\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        # Initialise Model\n",
    "        model_ft, input_size = self.initialise_model()\n",
    "        \n",
    "        # Set Device\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model_ft.to(device)\n",
    "        \n",
    "        # Set Optimizer \n",
    "        parameters =self.params_to_update(model_ft)\n",
    "        optimizer = optim.SGD(parameters, lr=self.learning_rate, momentum = self.momentum)\n",
    "        \n",
    "        # Set Criterion\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train Model\n",
    "        model_ft, val_hist = self.train_model(model_ft, device, self.dataloaders, self.dataset_sizes, \n",
    "                                              criterion, optimizer, num_epochs=5)\n",
    "        \n",
    "        #Progressive version\n",
    "#         list_val_hist = []\n",
    "#         for dataloader in train_dataloader:\n",
    "#             dataloaders = {\"train\": dataloader, \"val\": val_dataloader}\n",
    "#             dataset_sizes = {\"train\": len(dataloader.dataset), \"val\": len(val_dataloader.dataset)}\n",
    "#             model_ft, val_hist = self.train_model(model_ft, device, dataloaders, dataset_sizes,\n",
    "#                                                  criterion, optimizer, num_epochs=5)\n",
    "#             list_val_hist.append(val_hist)\n",
    "        \n",
    "        # Evaluate Model \n",
    "        accuracy = self.evaluate_model(model_ft, dataloaders, device)\n",
    "        \n",
    "        return model_ft, val_hist, accuracy\n",
    "        \n",
    "    \n",
    "    def params_to_update(self,model_ft):\n",
    "        if self.feature_extract: \n",
    "            params_to_update = []\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_update.append(param)\n",
    "        else:\n",
    "            params_to_update = model_ft.parameters()\n",
    "        \n",
    "        return params_to_update\n",
    "        \n",
    "    def set_parameter_requires_grad(self, model, feature_extracting):\n",
    "        \"\"\"\n",
    "        return: \n",
    "        \n",
    "        \"\"\"\n",
    "        if feature_extracting: \n",
    "            for param in model.parameters(): \n",
    "                param.requires_grad = False \n",
    "    \n",
    "    def initialise_model(self): \n",
    "        \"\"\"\n",
    "        return: model_ft, input_size\n",
    "        \n",
    "        \"\"\"\n",
    "        model_ft = None\n",
    "        input_size = 0 \n",
    "        \n",
    "        if self.model_name == \"resnet\":\n",
    "            model_ft = models.resnet50(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.fc.in_features\n",
    "            model_ft.fc = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        elif self.model_name == \"alexnet\":\n",
    "            model_ft = models.alexnet(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier[6].in_features\n",
    "            model_ft.classifier[6] = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        elif self.model_name == \"vgg\":\n",
    "            model_ft = models.vgg11_bn(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier[6].in_features\n",
    "            model_ft.classifier[6] = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "            \n",
    "        elif self.model_name == 'vggface':\n",
    "            model_ft = resnet50_scratch_dag(weights_path=\"resnet50_scratch_dag.pth\")\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier.in_channels\n",
    "            model_ft.classifer = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        self.model_ft = model_ft\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        return model_ft, input_size \n",
    "    \n",
    "    \n",
    "    #add dataset_size inside \n",
    "\n",
    "    def train_model(self,model,device,dataloaders, dataset_sizes, criterion, optimizer, num_epochs=25): \n",
    "        \"\"\"\n",
    "        return: model + val_acc_history\n",
    "        \n",
    "        \"\"\"\n",
    "        #start time \n",
    "        since = time.time()\n",
    "\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        val_acc_history = []\n",
    "\n",
    "        #for each epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "            #train and evaluate at current model at each epoch\n",
    "            for phase in [\"train\",\"val\"]:\n",
    "                if phase == \"train\":\n",
    "                    model.train() #set model to training mode\n",
    "                else:\n",
    "                    model.eval() #set model to evaluate mode\n",
    "\n",
    "                #keep track of loss \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    #zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    #forward prop \n",
    "                    #track history if only in train \n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        outputs = model(inputs) #predict\n",
    "                        _,preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        #backward + optimize only if training phase \n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    #stats \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "#                 if phase == \"train\":\n",
    "#                     scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss/dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() /dataset_sizes[phase]\n",
    "\n",
    "                #alternative to store in a list in order to visualise at the end \n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "                if phase == \"val\" and epoch_acc > best_acc: \n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model.state_dict())\n",
    "                if phase == \"val\":\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        model.load_state_dict(best_model)\n",
    "        \n",
    "        return model, val_acc_history \n",
    "    \n",
    "    def evaluate_model(self, model_ft, dataloaders, device):\n",
    "        correct = 0 \n",
    "        total = 0 \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders['val']: #change to test in real pipeline\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model_ft(inputs)\n",
    "                _,predicted = torch.max(outputs.data,1)\n",
    "                print(predicted)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        #Accuracy \n",
    "        accuracy = correct/total * 100\n",
    "        \n",
    "        # Unweighted Average F1 Score \n",
    "        \n",
    "        return correct/total * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
