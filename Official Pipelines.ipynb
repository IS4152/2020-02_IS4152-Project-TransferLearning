{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries \n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train-Test Split \n",
    "2. Data Preprocessing (change to black and white, getting the facial shape etc) \n",
    "3. Data Augmentation etc .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert AFF-Wild2 videos into images\n",
    "The focus is to convert selected frames of the video into images when there is a change in emotional label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoToImage(fileName):\n",
    "    label_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\annotations\\\\Train_Set\\\\\"\n",
    "    vid_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\videos\\\\Train_Set\\\\\"\n",
    "    img_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\images\\\\\"\n",
    "\n",
    "    file = open(label_data_dir+fileName, \"r\")\n",
    "    imgLabels = []\n",
    "    lastValue = 10\n",
    "    # Get labels from .txt files. Specifically focusing on getting the frames of video where label changes\n",
    "    for x in file:\n",
    "        if(x[0]!='N' and x[0]!='-'):\n",
    "            currentValue = int(x[0])\n",
    "            if lastValue==10 or lastValue!=currentValue:\n",
    "                imgLabels.append({'frame': count, 'label': currentValue})\n",
    "                lastValue = currentValue\n",
    "                count+=1\n",
    "\n",
    "    # Get specific frames and store as images\n",
    "    images = []\n",
    "    cap = cv2.VideoCapture(vid_data_dir+fileName[:-4]+\".mp4\")\n",
    "    for x in imgLabels:\n",
    "        cap.set(1, x['frame'])\n",
    "        ret, frame = cap.read()\n",
    "        if(ret):\n",
    "            images.append(frame)\n",
    "\n",
    "    # Store images into a separate folder\n",
    "    labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "    count = 0\n",
    "    for x in images:\n",
    "        img = Image.fromarray(x, 'RGB')\n",
    "        img.save(img_data_dir+labels[imgLabels[count]['label']]+'\\\\'+fileName[:-4]+'-'+str(count)+'.png')\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories (comment out if already created)\n",
    "os.mkdir(os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\images\")\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "for label in labels:\n",
    "    os.mkdir(os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\images\\\\\"+label)\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AFF Wild\\\\annotations\\\\Train_Set\\\\\"\n",
    "files = os.listdir(data_dir)\n",
    "for file in files:\n",
    "    videoToImage(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom transformers\n",
    "This transformer uses the haarcascade classifier to identify facial features, and crop the image to only the facial features <br/>\n",
    "This transformer also converts the image from RGB to a 3 channel grayscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        opencvImage = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        gray = cv2.cvtColor(opencvImage, cv2.COLOR_BGR2GRAY)\n",
    "        tripleGray = np.stack((gray,)*3, axis=-1)\n",
    "        faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "                tripleGray,\n",
    "                scaleFactor=1.3,\n",
    "                minNeighbors=3,\n",
    "                minSize=(30, 30)\n",
    "        )\n",
    "        for (x, y, w, h) in faces:\n",
    "            if len(faces)==1:\n",
    "                tripleGray[y:y+h, x:x+w]\n",
    "        if isinstance(self.output_size, int):\n",
    "            resized = cv2.resize(tripleGray, (self.output_size, self.output_size))\n",
    "        if isinstance(self.output_size, tuple):\n",
    "            resized = cv2.resize(tripleGray, self.output_size)\n",
    "        return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "There are 3 variations of the balanced datasets created here, the undersampled dataset, oversampled dataset and progressive dataset <br/>\n",
    "The balanced datasets are then transformed and converted into tensor files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampled dataset\n",
    "Create a balanced dataset by sampling image data to match the lowest label<br/>\n",
    "Final tensor files are then stored together to form the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CK+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\CK+\\\\sorted_dataset\\\\\"\n",
    "\n",
    "labels = ['anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "train_size =[]\n",
    "\n",
    "\n",
    "for x in range(6):\n",
    "    files = os.listdir(datadir+labels[x])\n",
    "    count = 0\n",
    "    train_size.append(math.floor(len(files)*0.8))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\undersampled_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "\n",
    "minLabel = min(train_size)\n",
    "\n",
    "for x in range(6):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"\\\\undersampled_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=minLabel)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"\\\\CK_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"\\\\CK_undersampled_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\CK_undersampled_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val']}\n",
    "\n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FER2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"\\\\datasets\\\\FER2013\\\\train\\\\\"\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "FERlabels = ['neutral', 'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+\"train\\\\\"+FERlabels[x])\n",
    "    count = 0\n",
    "    train_size.append(math.floor(len(files)*0.8))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            copyfile(datadir+\"train\\\\\"+FERlabels[x]+\"\\\\\"+file, datadir+\"\\\\FER_undersampled_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+\"train\\\\\"+FERlabels[x]+\"\\\\\"+file, datadir+\"\\\\FER_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "        \n",
    "minLabel = min(train_size)\n",
    "\n",
    "for x in range(7):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"\\\\FER_undersampled_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=minLabel)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"\\\\FER_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"\\\\FER_undersampled_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\FER2013\\\\FER_undersampled_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val']}\n",
    "\n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort AffectNet images into folders of respective emotion labels\n",
    "source_data_dir = os.getcwd() + \"\\\\datasets\\\\AffectNet\\\\Automatically_Annotated_compressed\\\\\"\n",
    "target_data_dir = os.getcwd() + \"\\\\datasets\\\\AffectNet_sorted\\\\\"\n",
    "df = pd.read_csv(source_data_dir+\"automatically_annotated.csv\")\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "\n",
    "    imageName = row['subDirectory_filePath'].split(\"/\")\n",
    "    if row['expression']==0:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\neutral\\\\\"+imageName[1])\n",
    "    if row['expression']==1:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\happy\\\\\"+imageName[1])\n",
    "    if row['expression']==2:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\sadness\\\\\"+imageName[1])\n",
    "    if row['expression']==3:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\suprise\\\\\"+imageName[1])\n",
    "    if row['expression']==4:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\fear\\\\\"+imageName[1])\n",
    "    if row['expression']==5:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\disgust\\\\\"+imageName[1])\n",
    "    if row['expression']==6:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\anger\\\\\"+imageName[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\AffectNet_sorted\\\\\"\n",
    "\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+labels[x])\n",
    "    count = 0\n",
    "    trainCount = 0\n",
    "    valCount=0\n",
    "    maxInput = 0\n",
    "    # Hardcoded minimum size of label\n",
    "    if len(files)>890:\n",
    "        maxInput = 890\n",
    "    else:\n",
    "        maxInput = len(files)\n",
    "    train_size.append(math.floor(maxInput*0.8))\n",
    "    valSample = np.random.choice(len(files), maxInput-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if valCount>=len(valSample) and trainCount>=train_size[x]:\n",
    "            break\n",
    "        if count in valSample and valCount<len(valSample):\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\AffectNet_undersampled_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            valCount+=1\n",
    "        if trainCount<train_size[x]:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\AffectNet_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            trainCount+=1\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AffectNet_sorted\\\\AffectNet_undersampled_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aff-Wild2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\\"\n",
    "\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+\"images\\\\\"+labels[x])\n",
    "    count = 0\n",
    "    train_size.append(math.floor(len(files)*0.6))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            if count%2==0:\n",
    "                copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"Aff_undersampled_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            else:\n",
    "                copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"Aff_undersampled_dataset\\\\test\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"Aff_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "        \n",
    "minLabel = min(train_size)\n",
    "\n",
    "for x in range(7):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"balanced_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=minLabel)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"Aff_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"Aff_undersampled_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AFF Wild\\\\Aff_undersampled_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val', 'test']}\n",
    "\n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\test\\\\\"\n",
    "for tensor in tqdm(image_datasets['test']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled dataset/Progressive dataset\n",
    "Create a balanced dataset by sampling image data to match the median label<br/>\n",
    "Final tensor files are either stored together (oversampled dataset) or separately (progressive dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CK+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\CK+\\\\sorted_dataset\\\\\"\n",
    "\n",
    "labels = ['anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "train_size =[]\n",
    "\n",
    "\n",
    "for x in range(6):\n",
    "    files = os.listdir(datadir+labels[x])\n",
    "    count = 0\n",
    "    train_size.append(math.floor(len(files)*0.8))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\CK_balanced_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\CK_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "\n",
    "median = math.ceil(statistics.median(train_size))\n",
    "print(\"Median: \"+str(median))\n",
    "\n",
    "for x in range(6):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"\\\\CK_balanced_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=median)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"\\\\CK_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"\\\\CK_balanced_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\CK+\\\\sorted_dataset\\\\CK_balanced_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val']}\n",
    "\n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\CK_balanced_dataset_tensors\\\\val\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\CK_balanced_dataset_tensors\\\\train\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FER2013\n",
    "FER2013 has slightly different naming conventions, so slight adjustments need to be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"\\\\datasets\\\\FER2013\\\\train\\\\\"\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "FERlabels = ['neutral', 'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+\"train\\\\\"+FERlabels[x])\n",
    "    count = 0\n",
    "    train_size.append(math.floor(len(files)*0.8))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            copyfile(datadir+\"train\\\\\"+FERlabels[x]+\"\\\\\"+file, datadir+\"\\\\FER_balanced_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+\"train\\\\\"+FERlabels[x]+\"\\\\\"+file, datadir+\"\\\\FER_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "        \n",
    "median = math.ceil(statistics.median(train_size))\n",
    "print(\"Median: \"+str(median))\n",
    "\n",
    "for x in range(7):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"\\\\FER_balanced_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=median)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"\\\\FER_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"\\\\FER_balanced_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\FER2013\\\\FER_balanced_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val']}\n",
    "\n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\FER_balanced_dataset_tensors\\\\val\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\FER_balanced_dataset_tensors\\\\train\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\AffectNet_sorted\\\\\"\n",
    "\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+labels[x])\n",
    "    count = 0\n",
    "    trainCount = 0\n",
    "    valCount=0\n",
    "    maxInput = 0\n",
    "    # Hardcoded median size of label\n",
    "    if len(files)>20854:\n",
    "        maxInput = 20854\n",
    "    else:\n",
    "        maxInput = len(files)\n",
    "    train_size.append(math.floor(maxInput*0.8))\n",
    "    valSample = np.random.choice(len(files), maxInput-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if valCount>=len(valSample) and trainCount>=train_size[x]:\n",
    "            break\n",
    "        if count in valSample and valCount<len(valSample):\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\AffectNet_balanced_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            valCount+=1\n",
    "        if trainCount<train_size[x]:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\AffectNet_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            trainCount+=1\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AffectNet_sorted\\\\AffectNet_balanced_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AffectNet_balanced_dataset_tensors\\\\val\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AffectNet_balanced_dataset_tensors\\\\train\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFF-Wild2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\\"\n",
    "\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+\"images\\\\\"+labels[x])\n",
    "    count = 0\n",
    "    train_size.append(math.floor(len(files)*0.6))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            if count%2==0:\n",
    "                copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"AFF_balanced_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            else:\n",
    "                copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"AFF_balanced_dataset\\\\test\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"AFF_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "        \n",
    "median = math.ceil(statistics.median(train_size))\n",
    "print(\"Median: \"+str(median))\n",
    "\n",
    "for x in range(7):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"AFF_balanced_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=median)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"AFF_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"AFF_balanced_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AFF Wild\\\\AFF_balanced_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val', 'test']}\n",
    "\n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AFF_balanced_dataset_tensors\\\\val\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AFF_balanced_dataset_tensors\\\\train\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AFF_balanced_dataset_tensors\\\\test\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\test\\\\\"\n",
    "for tensor in tqdm(image_datasets['test']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do: \n",
    "\n",
    "1. Add in F1 Metrics \n",
    "2. Figure out how to add tensorboard to get training diagnostics \n",
    "3. Progressive is implemented but need to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 trained from scratch\n",
    "class Resnet50_scratch_dag(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Resnet50_scratch_dag, self).__init__()\n",
    "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
    "                     'std': [1, 1, 1],\n",
    "                     'imageSize': [224, 224, 3]}\n",
    "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
    "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
    "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_3x3_relu = nn.ReLU()\n",
    "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_relu = nn.ReLU()\n",
    "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_3x3_relu = nn.ReLU()\n",
    "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_relu = nn.ReLU()\n",
    "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_3x3_relu = nn.ReLU()\n",
    "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_relu = nn.ReLU()\n",
    "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_3x3_relu = nn.ReLU()\n",
    "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_relu = nn.ReLU()\n",
    "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_3x3_relu = nn.ReLU()\n",
    "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_relu = nn.ReLU()\n",
    "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_3x3_relu = nn.ReLU()\n",
    "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_relu = nn.ReLU()\n",
    "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_3x3_relu = nn.ReLU()\n",
    "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_relu = nn.ReLU()\n",
    "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_3x3_relu = nn.ReLU()\n",
    "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_relu = nn.ReLU()\n",
    "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_3x3_relu = nn.ReLU()\n",
    "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_relu = nn.ReLU()\n",
    "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_3x3_relu = nn.ReLU()\n",
    "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_relu = nn.ReLU()\n",
    "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_3x3_relu = nn.ReLU()\n",
    "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_relu = nn.ReLU()\n",
    "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_3x3_relu = nn.ReLU()\n",
    "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_relu = nn.ReLU()\n",
    "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_3x3_relu = nn.ReLU()\n",
    "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_relu = nn.ReLU()\n",
    "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_3x3_relu = nn.ReLU()\n",
    "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_relu = nn.ReLU()\n",
    "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_3x3_relu = nn.ReLU()\n",
    "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_relu = nn.ReLU()\n",
    "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_3x3_relu = nn.ReLU()\n",
    "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_relu = nn.ReLU()\n",
    "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
    "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
    "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
    "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
    "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
    "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
    "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
    "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
    "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
    "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
    "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
    "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
    "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
    "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
    "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
    "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
    "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
    "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
    "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
    "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
    "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
    "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
    "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
    "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
    "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
    "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
    "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
    "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
    "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
    "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
    "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
    "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
    "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
    "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
    "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
    "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
    "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
    "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
    "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
    "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
    "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
    "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
    "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
    "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
    "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
    "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
    "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
    "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
    "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
    "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
    "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
    "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
    "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
    "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
    "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
    "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
    "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
    "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
    "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
    "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
    "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
    "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
    "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
    "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
    "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
    "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
    "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
    "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
    "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
    "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
    "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
    "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
    "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
    "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
    "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
    "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
    "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
    "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
    "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
    "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
    "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
    "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
    "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
    "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
    "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
    "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
    "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
    "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
    "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
    "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
    "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
    "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
    "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
    "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
    "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
    "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
    "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
    "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
    "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
    "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
    "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
    "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
    "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
    "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
    "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
    "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
    "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
    "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
    "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
    "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
    "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
    "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
    "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
    "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
    "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
    "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
    "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
    "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
    "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
    "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
    "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
    "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
    "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
    "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
    "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
    "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
    "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
    "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
    "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
    "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
    "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
    "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
    "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
    "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
    "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
    "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
    "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
    "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
    "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
    "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
    "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
    "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
    "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
    "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
    "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
    "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
    "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
    "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
    "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
    "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
    "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
    "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
    "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
    "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
    "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
    "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
    "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
    "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
    "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
    "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
    "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
    "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
    "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
    "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
    "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
    "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
    "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
    "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
    "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
    "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
    "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
    "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
    "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
    "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
    "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
    "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
    "        return classifier, pool5_7x7_s1\n",
    "\n",
    "def resnet50_scratch_dag(weights_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    load imported model instance\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): If set, loads model weights from the given path\n",
    "    \"\"\"\n",
    "    model = Resnet50_scratch_dag()\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialExpressionModel:\n",
    "    \n",
    "    def __init__(self, model_name, num_classes,dataloaders,dataset_sizes,lr, momentum,\n",
    "                 feature_extract=False, use_pretrained=True):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_extract = feature_extract\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.learning_rate = lr\n",
    "        self.momentum = momentum \n",
    "        self.model_ft = None\n",
    "        self.input_size = None\n",
    "        self.dataloaders = dataloaders\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        # Initialise Model\n",
    "        model_ft, input_size = self.initialise_model()\n",
    "        \n",
    "        # Set Device\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model_ft.to(device)\n",
    "        \n",
    "        # Set Optimizer \n",
    "        parameters =self.params_to_update(model_ft)\n",
    "        optimizer = optim.SGD(parameters, lr=self.learning_rate, momentum = self.momentum)\n",
    "        \n",
    "        # Set Criterion\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train Model\n",
    "        model_ft, val_hist,f1 = self.train_model(model_ft, device, self.dataloaders, self.dataset_sizes, \n",
    "                                              criterion, optimizer, num_epochs=50)\n",
    "        \n",
    "        #Progressive version\n",
    "#         list_val_hist = []\n",
    "#         for dataloader in train_dataloader:\n",
    "#             dataloaders = {\"train\": dataloader, \"val\": val_dataloader}\n",
    "#             dataset_sizes = {\"train\": len(dataloader.dataset), \"val\": len(val_dataloader.dataset)}\n",
    "#             model_ft, val_hist = self.train_model(model_ft, device, dataloaders, dataset_sizes,\n",
    "#                                                  criterion, optimizer, num_epochs=5)\n",
    "#             list_val_hist.append(val_hist)\n",
    "        \n",
    "        # Evaluate Model \n",
    "        accuracy = self.evaluate_model(model_ft, dataloaders, device)\n",
    "        \n",
    "        return model_ft, val_hist, accuracy\n",
    "        \n",
    "    \n",
    "    def params_to_update(self,model_ft):\n",
    "        if self.feature_extract: \n",
    "            params_to_update = []\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_update.append(param)\n",
    "        else:\n",
    "            params_to_update = model_ft.parameters()\n",
    "        \n",
    "        return params_to_update\n",
    "        \n",
    "    def set_parameter_requires_grad(self, model, feature_extracting):\n",
    "        \"\"\"\n",
    "        return: \n",
    "        \n",
    "        \"\"\"\n",
    "        if feature_extracting: \n",
    "            for param in model.parameters(): \n",
    "                param.requires_grad = False \n",
    "    \n",
    "    def initialise_model(self): \n",
    "        \"\"\"\n",
    "        return: model_ft, input_size\n",
    "        \n",
    "        \"\"\"\n",
    "        model_ft = None\n",
    "        input_size = 0 \n",
    "        \n",
    "        if self.model_name == \"resnet\":\n",
    "            model_ft = models.resnet50(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.fc.in_features\n",
    "            model_ft.fc = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        elif self.model_name == \"alexnet\":\n",
    "            model_ft = models.alexnet(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier[6].in_features\n",
    "            model_ft.classifier[6] = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        elif self.model_name == \"vgg\":\n",
    "            model_ft = models.vgg11_bn(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier[6].in_features\n",
    "            model_ft.classifier[6] = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "            \n",
    "        elif self.model_name == 'vggface':\n",
    "            model_ft = resnet50_scratch_dag(weights_path=\"resnet50_scratch_dag.pth\")\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier.in_channels\n",
    "            model_ft.classifer = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        self.model_ft = model_ft\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        return model_ft, input_size \n",
    "    \n",
    "    \n",
    "    #add dataset_size inside \n",
    "\n",
    "    def train_model(self,model,device,dataloaders, dataset_sizes, criterion, optimizer, num_epochs=25): \n",
    "        \"\"\"\n",
    "        return: model + val_acc_history\n",
    "        \n",
    "        \"\"\"\n",
    "        #start time \n",
    "        since = time.time()\n",
    "\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        val_acc_history = []\n",
    "\n",
    "        #for each epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "            #train and evaluate at current model at each epoch\n",
    "            for phase in [\"train\",\"val\"]:\n",
    "                if phase == \"train\":\n",
    "                    model.train() #set model to training mode\n",
    "                else:\n",
    "                    model.eval() #set model to evaluate mode\n",
    "\n",
    "                #keep track of loss \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    #zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    #forward prop \n",
    "                    #track history if only in train \n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        outputs = model(inputs) #predict\n",
    "                        _,preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        #backward + optimize only if training phase \n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    #stats \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "#                 if phase == \"train\":\n",
    "#                     scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss/dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() /dataset_sizes[phase]\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "                # Store Losses and Accuracy in Tensorboard\n",
    "                if phase == \"train\":\n",
    "                    writer_training.add_scalar(\"loss\", epoch_loss,epoch)\n",
    "                else:\n",
    "                    writer_validation.add_scalar(\"loss\", epoch_loss,epoch)\n",
    "                    writer_validation.add_scalar(\"accuracy\", epoch_acc,epoch)\n",
    "\n",
    "                if phase == \"val\" and epoch_acc > best_acc: \n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model.state_dict())\n",
    "                if phase == \"val\":\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        model.load_state_dict(best_model)\n",
    "        \n",
    "        return model, val_acc_history \n",
    "    \n",
    "    def evaluate_model(self, model_ft, dataloaders, device):\n",
    "        correct = 0 \n",
    "        total = 0 \n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders['test']: #change to test in real pipeline\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model_ft(inputs)\n",
    "                _,predicted = torch.max(outputs.data,1)\n",
    "#                 print(predicted)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "                all_preds.append(predicted.cpu().detach().numpy())\n",
    "                all_labels.append(labels.cpu().detach().numpy())\n",
    "                \n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "                \n",
    "        f1 = f1_score(all_labels, all_preds, average='macro' )\n",
    "        precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        #Accuracy \n",
    "        accuracy = correct/total * 100\n",
    "        \n",
    "        # Unweighted Average F1 Score \n",
    "        \n",
    "        return correct/total * 100, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment \n",
    "writer_training = SummaryWriter(\"./runs/ExperimentName/Training\")\n",
    "writer_validation = SummaryWriter(\"./runs/ExperimentName/Validation\")\n",
    "\n",
    "new_experiment = FacialExpressionModel(\"resnet\", 2,dataloaders, dataset_sizes, lr=0.001, momentum=0.9,\n",
    "                 feature_extract=False, use_pretrained=True) #add in epochs as well\n",
    "model_ft, val_hist, accuracy = new_experiment.train()\n",
    "\n",
    "writer_training.flush()\n",
    "writer_validation.flush()\n",
    "writer_training.close()\n",
    "writer_training.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extension and start TensorBoard for kaggle\n",
    "\n",
    "%load_ext tensorboard.notebook\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
