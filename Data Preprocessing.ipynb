{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import statistics\n",
    "import imblearn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from shutil import copyfile, move\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data cleaning and preparation, we did the following\n",
    "1. Extract image data from AFF-Wild2 dataset\n",
    "2. Transform data to fit our model\n",
    "3. Create different variations of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert AFF-Wild2 videos into images\n",
    "As the original AFF-Wild2 dataset are a series of videos, the focus is to convert selected frames of the video into images. <br/> We used frames where there was a change in emotion label as our selection criteria, to reduce similarities between images within the same label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract individual frames as images to create dataset\n",
    "def videoToImage(fileName):\n",
    "    label_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\annotations\\\\Train_Set\\\\\"\n",
    "    vid_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\videos\\\\Train_Set\\\\\"\n",
    "    img_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\images\\\\\"\n",
    "\n",
    "    file = open(label_data_dir+fileName, \"r\")\n",
    "    imgLabels = []\n",
    "    lastValue = 10\n",
    "    # Get labels from .txt files. Specifically focusing on getting the frames of video where label changes\n",
    "    for x in file:\n",
    "        if(x[0]!='N' and x[0]!='-'):\n",
    "            currentValue = int(x[0])\n",
    "            if lastValue==10 or lastValue!=currentValue:\n",
    "                imgLabels.append({'frame': count, 'label': currentValue})\n",
    "                lastValue = currentValue\n",
    "                count+=1\n",
    "\n",
    "    # Get specific frames and store as images\n",
    "    images = []\n",
    "    cap = cv2.VideoCapture(vid_data_dir+fileName[:-4]+\".mp4\")\n",
    "    for x in imgLabels:\n",
    "        cap.set(1, x['frame'])\n",
    "        ret, frame = cap.read()\n",
    "        if(ret):\n",
    "            images.append(frame)\n",
    "\n",
    "    # Store images into a separate folder\n",
    "    labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "    count = 0\n",
    "    for x in images:\n",
    "        img = Image.fromarray(x, 'RGB')\n",
    "        img.save(img_data_dir+labels[imgLabels[count]['label']]+'\\\\'+fileName[:-4]+'-'+str(count)+'.png')\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories (comment out if already created)\n",
    "os.mkdir(os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\images\")\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "for label in labels:\n",
    "    os.mkdir(os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\images\\\\\"+label)\n",
    "\n",
    "# Loop through all video files\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AFF Wild\\\\annotations\\\\Train_Set\\\\\"\n",
    "files = os.listdir(data_dir)\n",
    "for file in files:\n",
    "    videoToImage(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "Our data augmentation steps focus on transforming the data to fit our model and creating variations in the dataset for our experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom transformers\n",
    "This transformer uses the haarcascade classifier to identify facial features, and crop the image to only the facial features <br/>\n",
    "This transformer also converts the image from RGB to a 3 channel grayscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        opencvImage = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        gray = cv2.cvtColor(opencvImage, cv2.COLOR_BGR2GRAY)\n",
    "        tripleGray = np.stack((gray,)*3, axis=-1)\n",
    "        faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "                tripleGray,\n",
    "                scaleFactor=1.3,\n",
    "                minNeighbors=3,\n",
    "                minSize=(30, 30)\n",
    "        )\n",
    "        for (x, y, w, h) in faces:\n",
    "            if len(faces)==1:\n",
    "                tripleGray[y:y+h, x:x+w]\n",
    "        if isinstance(self.output_size, int):\n",
    "            resized = cv2.resize(tripleGray, (self.output_size, self.output_size))\n",
    "        if isinstance(self.output_size, tuple):\n",
    "            resized = cv2.resize(tripleGray, self.output_size)\n",
    "        return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "There are 3 variations of the balanced datasets created here, the undersampled dataset, oversampled dataset and progressive dataset <br/>\n",
    "The balanced datasets are then transformed and converted into tensor files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampled dataset\n",
    "Create a balanced dataset by sampling image data to match the lowest label<br/>\n",
    "Final tensor files are then stored together to form the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CK+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\CK+\\\\sorted_dataset\\\\\"\n",
    "\n",
    "labels = ['anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "train_size =[]\n",
    "\n",
    "\n",
    "for x in range(6):\n",
    "    files = os.listdir(datadir+labels[x])\n",
    "    count = 0\n",
    "    # Create a 80/20 train/validation split\n",
    "    train_size.append(math.floor(len(files)*0.8))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\undersampled_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "\n",
    "# Obtain size of label with lowest data\n",
    "minLabel = min(train_size)\n",
    "\n",
    "for x in range(6):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"\\\\undersampled_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=minLabel)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"\\\\CK_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"\\\\CK_undersampled_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\CK_undersampled_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val']}\n",
    "\n",
    "# Convert our augmented images and store as tensor files\n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FER2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"\\\\datasets\\\\FER2013\\\\train\\\\\"\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "FERlabels = ['neutral', 'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+\"train\\\\\"+FERlabels[x])\n",
    "    count = 0\n",
    "    # Create a 80/20 train/validation split\n",
    "    train_size.append(math.floor(len(files)*0.8))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            copyfile(datadir+\"train\\\\\"+FERlabels[x]+\"\\\\\"+file, datadir+\"\\\\FER_undersampled_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+\"train\\\\\"+FERlabels[x]+\"\\\\\"+file, datadir+\"\\\\FER_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "        \n",
    "# Obtain size of label with lowest data\n",
    "minLabel = min(train_size)\n",
    "\n",
    "for x in range(7):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"\\\\FER_undersampled_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=minLabel)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"\\\\FER_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"\\\\FER_undersampled_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\FER2013\\\\FER_undersampled_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val']}\n",
    "\n",
    "# Convert our augmented images and store as tensor files\n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort AffectNet images into folders of respective emotion labels\n",
    "source_data_dir = os.getcwd() + \"\\\\datasets\\\\AffectNet\\\\Automatically_Annotated_compressed\\\\\"\n",
    "target_data_dir = os.getcwd() + \"\\\\datasets\\\\AffectNet_sorted\\\\\"\n",
    "df = pd.read_csv(source_data_dir+\"automatically_annotated.csv\")\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "\n",
    "    imageName = row['subDirectory_filePath'].split(\"/\")\n",
    "    if row['expression']==0:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\neutral\\\\\"+imageName[1])\n",
    "    if row['expression']==1:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\happy\\\\\"+imageName[1])\n",
    "    if row['expression']==2:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\sadness\\\\\"+imageName[1])\n",
    "    if row['expression']==3:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\suprise\\\\\"+imageName[1])\n",
    "    if row['expression']==4:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\fear\\\\\"+imageName[1])\n",
    "    if row['expression']==5:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\disgust\\\\\"+imageName[1])\n",
    "    if row['expression']==6:\n",
    "        copyfile(source_data_dir+\"\\\\Automatically_Annotated\\\\Automatically_Annotated_images\\\\\"+row['subDirectory_filePath'], target_data_dir+\"\\\\anger\\\\\"+imageName[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\AffectNet_sorted\\\\\"\n",
    "\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+labels[x])\n",
    "    count = 0\n",
    "    trainCount = 0\n",
    "    valCount=0\n",
    "    maxInput = 0\n",
    "    # Hardcoded minimum size of label\n",
    "    if len(files)>890:\n",
    "        maxInput = 890\n",
    "    else:\n",
    "        maxInput = len(files)\n",
    "    # Create a 80/20 train/validation split\n",
    "    train_size.append(math.floor(maxInput*0.8))\n",
    "    valSample = np.random.choice(len(files), maxInput-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if valCount>=len(valSample) and trainCount>=train_size[x]:\n",
    "            break\n",
    "        if count in valSample and valCount<len(valSample):\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\AffectNet_undersampled_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            valCount+=1\n",
    "        if trainCount<train_size[x]:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\AffectNet_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            trainCount+=1\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AffectNet_sorted\\\\AffectNet_undersampled_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Convert our augmented images and store as tensor files\n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aff-Wild2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\\"\n",
    "\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+\"images\\\\\"+labels[x])\n",
    "    count = 0\n",
    "    # Create a 60/20/20 train/test/validation split\n",
    "    train_size.append(math.floor(len(files)*0.6))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            if count%2==0:\n",
    "                copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"Aff_undersampled_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            else:\n",
    "                copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"Aff_undersampled_dataset\\\\test\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"Aff_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "\n",
    "# Obtain size of label with lowest data\n",
    "minLabel = min(train_size)\n",
    "\n",
    "for x in range(7):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"balanced_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=minLabel)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"Aff_undersampled_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"Aff_undersampled_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AFF Wild\\\\Aff_undersampled_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val', 'test']}\n",
    "\n",
    "# Convert our augmented images and store as tensor files\n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\undersampled_dataset_tensors\\\\test\\\\\"\n",
    "for tensor in tqdm(image_datasets['test']):\n",
    "    torch.save(tensor, data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled dataset/Progressive dataset\n",
    "Create a balanced dataset by sampling image data to match the median label<br/>\n",
    "Final tensor files are either stored together (oversampled dataset) or separately (progressive dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CK+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\CK+\\\\sorted_dataset\\\\\"\n",
    "\n",
    "labels = ['anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "train_size =[]\n",
    "\n",
    "\n",
    "for x in range(6):\n",
    "    files = os.listdir(datadir+labels[x])\n",
    "    count = 0\n",
    "    # Create a 80/20 train/validation split\n",
    "    train_size.append(math.floor(len(files)*0.8))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\CK_balanced_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\CK_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "\n",
    "# Obtain size of label with median amount data\n",
    "median = math.ceil(statistics.median(train_size))\n",
    "\n",
    "for x in range(6):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"\\\\CK_balanced_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=median)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"\\\\CK_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"\\\\CK_balanced_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\CK+\\\\sorted_dataset\\\\CK_balanced_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val']}\n",
    "\n",
    "# Convert our augmented images and store as tensor files\n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\CK_balanced_dataset_tensors\\\\val\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\CK_balanced_dataset_tensors\\\\train\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FER2013\n",
    "FER2013 has slightly different naming conventions, so slight adjustments need to be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"\\\\datasets\\\\FER2013\\\\train\\\\\"\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "FERlabels = ['neutral', 'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+\"train\\\\\"+FERlabels[x])\n",
    "    count = 0\n",
    "    # Create a 80/20 train/validation split\n",
    "    train_size.append(math.floor(len(files)*0.8))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            copyfile(datadir+\"train\\\\\"+FERlabels[x]+\"\\\\\"+file, datadir+\"\\\\FER_balanced_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+\"train\\\\\"+FERlabels[x]+\"\\\\\"+file, datadir+\"\\\\FER_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "\n",
    "# Obtain size of label with median amount data\n",
    "median = math.ceil(statistics.median(train_size))\n",
    "\n",
    "for x in range(7):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"\\\\FER_balanced_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=median)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"\\\\FER_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"\\\\FER_balanced_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\FER2013\\\\FER_balanced_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val']}\n",
    "\n",
    "# Convert our augmented images and store as tensor files\n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\FER_balanced_dataset_tensors\\\\val\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\FER_balanced_dataset_tensors\\\\train\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\AffectNet_sorted\\\\\"\n",
    "\n",
    "labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+labels[x])\n",
    "    count = 0\n",
    "    trainCount = 0\n",
    "    valCount=0\n",
    "    maxInput = 0\n",
    "    # Hardcoded median size of label\n",
    "    if len(files)>20854:\n",
    "        maxInput = 20854\n",
    "    else:\n",
    "        maxInput = len(files)\n",
    "    # Create a 80/20 train/validation split\n",
    "    train_size.append(math.floor(maxInput*0.8))\n",
    "    valSample = np.random.choice(len(files), maxInput-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if valCount>=len(valSample) and trainCount>=train_size[x]:\n",
    "            break\n",
    "        if count in valSample and valCount<len(valSample):\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\AffectNet_balanced_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            valCount+=1\n",
    "        if trainCount<train_size[x]:\n",
    "            copyfile(datadir+labels[x]+\"\\\\\"+file, datadir+\"\\\\AffectNet_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            trainCount+=1\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AffectNet_sorted\\\\AffectNet_balanced_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Convert our augmented images and store as tensor files\n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AffectNet_balanced_dataset_tensors\\\\val\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AffectNet_balanced_dataset_tensors\\\\train\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFF-Wild2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\\"\n",
    "\n",
    "train_size=[]\n",
    "\n",
    "for x in range(7):\n",
    "    files = os.listdir(datadir+\"images\\\\\"+labels[x])\n",
    "    count = 0\n",
    "    # Create a 60/20/20 train/test/validation split\n",
    "    train_size.append(math.floor(len(files)*0.6))\n",
    "    valSample = np.random.choice(len(files), len(files)-train_size[x], replace=False)\n",
    "    \n",
    "    for file in files:\n",
    "        if count in valSample:\n",
    "            if count%2==0:\n",
    "                copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"AFF_balanced_dataset\\\\val\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "            else:\n",
    "                copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"AFF_balanced_dataset\\\\test\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        else:\n",
    "            copyfile(datadir+\"images\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"AFF_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file)\n",
    "        count+=1\n",
    "        \n",
    "# Obtain size of label with median amount data\n",
    "median = math.ceil(statistics.median(train_size))\n",
    "for x in range(7):\n",
    "    count=0\n",
    "    files = os.listdir(datadir+\"AFF_balanced_dataset\\\\train\\\\\"+labels[x])\n",
    "    trainSample = np.random.randint(low=0, high=train_size[x], size=median)\n",
    "    for file in files:\n",
    "        freq = np.count_nonzero(trainSample == count)\n",
    "        if freq>0:\n",
    "            for i in range(freq):\n",
    "                copyfile(datadir+\"AFF_balanced_dataset\\\\train\\\\\"+labels[x]+\"\\\\\"+file, datadir+\"AFF_balanced_dataset\\\\train_sampled\\\\\"+labels[x]+\"\\\\\"+str(i)+file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our data\n",
    "data_transforms = {\n",
    "    'train_sampled': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AFF Wild\\\\AFF_balanced_dataset\\\\\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train_sampled', 'val', 'test']}\n",
    "\n",
    "# Convert our augmented images and store as tensor files\n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AFF_balanced_dataset_tensors\\\\val\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\val\\\\\"\n",
    "for tensor in tqdm(image_datasets['val']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AFF_balanced_dataset_tensors\\\\train\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\train\\\\\"\n",
    "for tensor in tqdm(image_datasets['train_sampled']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1\n",
    "    \n",
    "count=0\n",
    "progressive_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\AFF_balanced_dataset_tensors\\\\test\\\\\"\n",
    "oversampled_data_dir = os.getcwd() + \"\\\\datasets\\\\sorted_dataset\\\\combined_balanced_dataset_tensors\\\\test\\\\\"\n",
    "for tensor in tqdm(image_datasets['test']):\n",
    "    torch.save(tensor, progressive_data_dir+str(count)+'.pt')\n",
    "    torch.save(tensor, oversampled_data_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
