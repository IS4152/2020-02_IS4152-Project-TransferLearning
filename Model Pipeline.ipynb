{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "import copy\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "Our datasets are stored as a series of tensor files, to speed up our model training.<br/>\n",
    "Below are our methods to load the progressive, oversampled and undersampled datasets respectively, to prepare for training<br/>\n",
    "Do note that many of the filepaths are relative to kaggle default pathing, as we ran our model pipeline on Kaggle to utilise GPU resources available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "datapath1 = \"/kaggle/input/progressive-1\"\n",
    "datapath2 = \"/kaggle/input/progressive-2\"\n",
    "dataloaders = []\n",
    "datasets1 = os.listdir(datapath1)\n",
    "dataset2 = os.listdir(datapath2)\n",
    "normal = [\"CK_balanced_dataset_tensors\", \"FER_balanced_dataset_tensors\", \"AFF_balanced_dataset_tensors\"]\n",
    "\n",
    "for dataset in tqdm(normal):\n",
    "    dataset_path = os.path.join(datapath1, dataset,dataset)\n",
    "    if dataset != \"AFF_balanced_dataset_tensors\": \n",
    "        stages = [\"train\", \"val\"]\n",
    "    else: \n",
    "        stages = [\"train\", \"val\", \"test\"]\n",
    "    dataloader={stage:[] for stage in stages}\n",
    "    for stage in stages: \n",
    "        stagepath = os.path.join(dataset_path,stage)\n",
    "        data = [os.path.join(stagepath,f)\n",
    "            for f in listdir(stagepath) if isfile(join(stagepath, f))]\n",
    "        dataloader[stage] = data\n",
    "    dataloaders.append(dataloader)\n",
    "    \n",
    "#affectnet\n",
    "dataloader={stage:[] for stage in [\"train\",\"val\"]}\n",
    "for dataset in tqdm(datasets1):\n",
    "    dataset_path = os.path.join(datapath1, dataset, dataset)\n",
    "    \n",
    "    if dataset not in normal:\n",
    "        if dataset == \"AffectNet_balanced_dataset_tensors_val\":\n",
    "            data = [os.path.join(dataset_path,f)\n",
    "                for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "            dataloader[\"val\"].extend(data)\n",
    "        else:\n",
    "            data = [os.path.join(dataset_path,f)\n",
    "                for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "            dataloader[\"train\"].extend(data)\n",
    "\n",
    "for dataset in tqdm(datasets2):\n",
    "    dataset_path = os.path.join(datapath2, dataset, dataset)\n",
    "    if dataset not in normal:\n",
    "        if dataset == \"AffectNet_balanced_dataset_tensors_val\":\n",
    "            data = [os.path.join(dataset_path,f)\n",
    "                for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "            dataloader[\"val\"].extend(data)\n",
    "        else:\n",
    "            data = [os.path.join(dataset_path,f)\n",
    "                for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "            dataloader[\"train\"].extend(data)\n",
    "\n",
    "dataloaders = [dataloaders[0], dataloaders[1], dataloader,dataloaders[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "datapath1 = \"/kaggle/input/progressive-1\"\n",
    "datapath2 = \"/kaggle/input/progressive-2\"\n",
    "dataloaders = {stage:[] for stage in [\"train\",\"val\",\"test\"]}\n",
    "datasets1 = os.listdir(datapath1)\n",
    "normal = [\"CK_balanced_dataset_tensors\", \"FER_balanced_dataset_tensors\", \"AFF_balanced_dataset_tensors\"]\n",
    "\n",
    "for dataset in datasets1:\n",
    "    dataset_path = os.path.join(datapath1, dataset, dataset)\n",
    "    print(dataset_path)\n",
    "    if dataset not in normal: \n",
    "        if dataset == \"AffectNet_balanced_dataset_tensors_val\":\n",
    "            data = [os.path.join(dataset_path,f)\n",
    "                for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "            print(data)\n",
    "            dataloaders[\"val\"].extend(data)\n",
    "        else:\n",
    "            data = [os.path.join(dataset_path,f)\n",
    "                for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "            dataloaders[\"train\"].extend(data)\n",
    "            \n",
    "    else:\n",
    "        if dataset != \"AFF_balanced_dataset_tensors\": \n",
    "            stages = [\"train\", \"val\"]\n",
    "        else: \n",
    "            stages = [\"train\", \"val\", \"test\"]\n",
    "        for stage in stages: \n",
    "            stagepath = os.path.join(dataset_path,stage)\n",
    "            data = [os.path.join(stagepath,f)\n",
    "                for f in listdir(stagepath) if isfile(join(stagepath, f))]\n",
    "            dataloaders[stage].extend(data)\n",
    "\n",
    "datasets2 = os.listdir(datapath2)\n",
    "for dataset in datasets2:\n",
    "    dataset_path = os.path.join(datapath2, dataset, dataset)\n",
    "    print(dataset_path)\n",
    "    if dataset not in normal: \n",
    "        if dataset == \"AffectNet_balanced_dataset_tensors_val\":\n",
    "            data = [os.path.join(dataset_path,f)\n",
    "                for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "            dataloaders[\"val\"].extend(data)\n",
    "        else:\n",
    "            data = [os.path.join(dataset_path,f)\n",
    "                for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "            dataloaders[\"train\"].extend(data)\n",
    "            \n",
    "    else:\n",
    "        if dataset != \"AFF_balanced_dataset_tensors\": \n",
    "            stages = [\"train\", \"val\"]\n",
    "        else: \n",
    "            stages = [\"train\", \"val\", \"test\"]\n",
    "        for stage in stages: \n",
    "            stagepath = os.path.join(dataset_path,stage)\n",
    "            data = [os.path.join(stagepath,f)\n",
    "                for f in listdir(stagepath) if isfile(join(stagepath, f))]\n",
    "            dataloaders[stage].extend(data)\n",
    "            \n",
    "dataset_sizes = {x: len(dataloaders[x]) for x in stages}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "datapath = \"/kaggle/input/combined-facial-expression/balanced_dataset_tensors/balanced_dataset_tensors\"\n",
    "stages = [\"train\", \"test\", \"val\"]\n",
    "dataloaders={stage:[] for stage in stages}\n",
    "\n",
    "for stage in tqdm(stages): \n",
    "    stagepath = os.path.join(datapath, stage)\n",
    "    data = [os.path.join(stagepath,f)\n",
    "            for f in listdir(stagepath) if isfile(join(stagepath, f))]\n",
    "    dataloaders[stage]=data\n",
    "    \n",
    "dataset_sizes = {x: len(dataloaders[x]) for x in stages}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation for VGGFace and VGGFace2\n",
    "As both these models are not within the standard pytorch libraries, the VGGFace and VGGFace2 models need to be initialized manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGFace2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 trained from scratch (VGGFace2)\n",
    "class Resnet50_scratch_dag(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Resnet50_scratch_dag, self).__init__()\n",
    "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
    "                     'std': [1, 1, 1],\n",
    "                     'imageSize': [224, 224, 3]}\n",
    "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
    "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
    "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_3x3_relu = nn.ReLU()\n",
    "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_1_relu = nn.ReLU()\n",
    "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_3x3_relu = nn.ReLU()\n",
    "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_2_relu = nn.ReLU()\n",
    "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_3x3_relu = nn.ReLU()\n",
    "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2_3_relu = nn.ReLU()\n",
    "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_3x3_relu = nn.ReLU()\n",
    "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_1_relu = nn.ReLU()\n",
    "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_3x3_relu = nn.ReLU()\n",
    "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_2_relu = nn.ReLU()\n",
    "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_3x3_relu = nn.ReLU()\n",
    "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_3_relu = nn.ReLU()\n",
    "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_3x3_relu = nn.ReLU()\n",
    "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv3_4_relu = nn.ReLU()\n",
    "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_3x3_relu = nn.ReLU()\n",
    "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_1_relu = nn.ReLU()\n",
    "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_3x3_relu = nn.ReLU()\n",
    "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_2_relu = nn.ReLU()\n",
    "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_3x3_relu = nn.ReLU()\n",
    "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_3_relu = nn.ReLU()\n",
    "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_3x3_relu = nn.ReLU()\n",
    "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_4_relu = nn.ReLU()\n",
    "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_3x3_relu = nn.ReLU()\n",
    "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_5_relu = nn.ReLU()\n",
    "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_3x3_relu = nn.ReLU()\n",
    "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4_6_relu = nn.ReLU()\n",
    "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_3x3_relu = nn.ReLU()\n",
    "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
    "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_1_relu = nn.ReLU()\n",
    "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_3x3_relu = nn.ReLU()\n",
    "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_2_relu = nn.ReLU()\n",
    "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
    "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_3x3_relu = nn.ReLU()\n",
    "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
    "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv5_3_relu = nn.ReLU()\n",
    "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
    "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
    "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
    "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
    "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
    "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
    "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
    "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
    "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
    "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
    "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
    "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
    "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
    "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
    "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
    "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
    "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
    "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
    "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
    "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
    "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
    "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
    "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
    "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
    "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
    "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
    "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
    "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
    "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
    "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
    "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
    "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
    "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
    "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
    "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
    "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
    "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
    "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
    "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
    "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
    "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
    "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
    "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
    "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
    "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
    "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
    "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
    "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
    "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
    "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
    "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
    "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
    "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
    "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
    "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
    "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
    "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
    "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
    "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
    "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
    "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
    "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
    "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
    "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
    "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
    "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
    "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
    "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
    "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
    "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
    "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
    "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
    "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
    "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
    "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
    "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
    "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
    "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
    "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
    "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
    "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
    "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
    "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
    "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
    "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
    "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
    "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
    "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
    "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
    "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
    "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
    "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
    "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
    "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
    "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
    "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
    "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
    "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
    "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
    "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
    "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
    "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
    "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
    "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
    "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
    "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
    "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
    "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
    "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
    "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
    "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
    "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
    "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
    "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
    "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
    "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
    "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
    "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
    "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
    "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
    "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
    "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
    "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
    "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
    "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
    "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
    "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
    "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
    "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
    "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
    "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
    "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
    "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
    "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
    "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
    "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
    "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
    "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
    "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
    "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
    "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
    "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
    "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
    "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
    "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
    "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
    "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
    "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
    "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
    "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
    "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
    "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
    "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
    "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
    "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
    "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
    "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
    "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
    "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
    "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
    "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
    "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
    "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
    "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
    "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
    "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
    "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
    "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
    "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
    "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
    "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
    "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
    "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
    "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
    "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
    "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
    "        return classifier, pool5_7x7_s1\n",
    "\n",
    "def resnet50_scratch_dag(weights_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    load imported model instance\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): If set, loads model weights from the given path\n",
    "    \"\"\"\n",
    "    model = Resnet50_scratch_dag()\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGFace\n",
    "class Vgg_face_dag(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Vgg_face_dag, self).__init__()\n",
    "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
    "                     'std': [1, 1, 1],\n",
    "                     'imageSize': [224, 224, 3]}\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        x1 = self.conv1_1(x0)\n",
    "        x2 = self.relu1_1(x1)\n",
    "        x3 = self.conv1_2(x2)\n",
    "        x4 = self.relu1_2(x3)\n",
    "        x5 = self.pool1(x4)\n",
    "        x6 = self.conv2_1(x5)\n",
    "        x7 = self.relu2_1(x6)\n",
    "        x8 = self.conv2_2(x7)\n",
    "        x9 = self.relu2_2(x8)\n",
    "        x10 = self.pool2(x9)\n",
    "        x11 = self.conv3_1(x10)\n",
    "        x12 = self.relu3_1(x11)\n",
    "        x13 = self.conv3_2(x12)\n",
    "        x14 = self.relu3_2(x13)\n",
    "        x15 = self.conv3_3(x14)\n",
    "        x16 = self.relu3_3(x15)\n",
    "        x17 = self.pool3(x16)\n",
    "        x18 = self.conv4_1(x17)\n",
    "        x19 = self.relu4_1(x18)\n",
    "        x20 = self.conv4_2(x19)\n",
    "        x21 = self.relu4_2(x20)\n",
    "        x22 = self.conv4_3(x21)\n",
    "        x23 = self.relu4_3(x22)\n",
    "        x24 = self.pool4(x23)\n",
    "        x25 = self.conv5_1(x24)\n",
    "        x26 = self.relu5_1(x25)\n",
    "        x27 = self.conv5_2(x26)\n",
    "        x28 = self.relu5_2(x27)\n",
    "        x29 = self.conv5_3(x28)\n",
    "        x30 = self.relu5_3(x29)\n",
    "        x31_preflatten = self.pool5(x30)\n",
    "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
    "        x32 = self.fc6(x31)\n",
    "        x33 = self.relu6(x32)\n",
    "        x34 = self.dropout6(x33)\n",
    "        x35 = self.fc7(x34)\n",
    "        x36 = self.relu7(x35)\n",
    "        x37 = self.dropout7(x36)\n",
    "        x38 = self.fc8(x37)\n",
    "        return x38\n",
    "\n",
    "def vgg_face_dag(weights_path=None, **kwargs):\n",
    "    \"\"\"\n",
    "    load imported model instance\n",
    "\n",
    "    Args:\n",
    "        weights_path (str): If set, loads model weights from the given path\n",
    "    \"\"\"\n",
    "    model = Vgg_face_dag()\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Model\n",
    "Our model pipeline, which trains, test and validate our models according to the specified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialExpressionModel:\n",
    "    \n",
    "    def __init__(self, model_name, num_classes,dataloaders,dataset_sizes,lr, momentum,\n",
    "                 feature_extract=False, use_pretrained=True, progressive=False):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_extract = feature_extract\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.learning_rate = lr\n",
    "        self.momentum = momentum \n",
    "        self.model_ft = None\n",
    "        self.input_size = None\n",
    "        self.dataloaders = dataloaders\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "        self.progressive = progressive\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        # Initialise Model\n",
    "        model_ft, input_size = self.initialise_model()\n",
    "        \n",
    "        # Set Device\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model_ft.to(device)\n",
    "        \n",
    "        # Set Optimizer \n",
    "        parameters =self.params_to_update(model_ft)\n",
    "        optimizer = optim.SGD(parameters, lr=self.learning_rate, momentum = self.momentum)\n",
    "        \n",
    "        # Set Criterion\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Set LR Scheduler \n",
    "#         exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        \n",
    "        # Train Model\n",
    "        if self.progressive:\n",
    "            counter = 0\n",
    "            for dataloader in self.dataloaders:\n",
    "                print(\"Dataset\" + str(counter))\n",
    "                curr_dataloader = dataloaders[counter]\n",
    "                curr_dataset_size = {x: len(curr_dataloader[x]) for x in [\"train\",\"val\"]}\n",
    "                print(curr_dataset_size)\n",
    "                model_ft, val_hist,train_d,val_d = self.train_model(model_ft, device, curr_dataloader, curr_dataset_size,\n",
    "                                                     criterion, optimizer, num_epochs=4)  \n",
    "                \n",
    "                # Save JSON File \n",
    "                path = \"./runs/\" + self.model_name + \"/\" + str(counter)\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "\n",
    "                train_json = json.dumps(train_d)\n",
    "                val_json = json.dumps(val_d)\n",
    "\n",
    "                train_file = open(path+\"/train.json\",\"w\")\n",
    "                train_file.write(train_json)\n",
    "                train_file.close()\n",
    "\n",
    "                val_file = open(path+\"/val.json\",\"w\")\n",
    "                val_file.write(val_json)\n",
    "                val_file.close()\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            model_ft, val_hist, train_d, val_d = self.train_model(model_ft, device, self.dataloaders, self.dataset_sizes, \n",
    "                                              criterion, optimizer, num_epochs=10)\n",
    "        \n",
    "        \n",
    "        # Evaluate Model \n",
    "        accuracy, f1 = self.evaluate_model(model_ft, dataloaders, device)\n",
    "        \n",
    "        return model_ft, val_hist, train_d, val_d, accuracy, f1\n",
    "      \n",
    "    \n",
    "    def params_to_update(self,model_ft):\n",
    "        if self.feature_extract: \n",
    "            params_to_update = []\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_update.append(param)\n",
    "        else:\n",
    "            params_to_update = model_ft.parameters()\n",
    "        \n",
    "        \n",
    "        return params_to_update\n",
    "        \n",
    "    def set_parameter_requires_grad(self, model, feature_extracting):\n",
    "        \"\"\"\n",
    "        return: \n",
    "        \n",
    "        \"\"\"\n",
    "        if feature_extracting: \n",
    "            for param in model.parameters(): \n",
    "                param.requires_grad = False \n",
    "    \n",
    "    def initialise_model(self): \n",
    "        \"\"\"\n",
    "        return: model_ft, input_size\n",
    "        \n",
    "        \"\"\"\n",
    "        model_ft = None\n",
    "        input_size = 0 \n",
    "        \n",
    "        if self.model_name == \"resnet\":\n",
    "            model_ft = models.resnet18(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.fc.in_features\n",
    "            model_ft.fc = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        elif self.model_name == \"alexnet\":\n",
    "            model_ft = models.alexnet(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier[6].in_features\n",
    "            model_ft.classifier[6] = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        elif self.model_name == \"vgg\":\n",
    "            model_ft = models.vgg11(pretrained=self.use_pretrained)\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier[6].in_features\n",
    "            model_ft.classifier[6] = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "            \n",
    "        elif self.model_name == 'vggface2':\n",
    "            model_ft = resnet50_scratch_dag(weights_path=\"/kaggle/input/vggface2/resnet50_scratch_dag.pth\")\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.classifier.in_channels\n",
    "            model_ft.classifier = nn.Conv2d(num_ftrs, self.num_classes, kernel_size=[1, 1], stride=(1, 1))\n",
    "            input_size = 224\n",
    "            \n",
    "        elif self.model_name == 'vggface':\n",
    "            model_ft = vgg_face_dag(weights_path=\"/kaggle/input/vggfaceoriginal/vgg_face_dag.pth\")\n",
    "            self.set_parameter_requires_grad(model_ft, self.feature_extract)\n",
    "            num_ftrs = model_ft.fc8.in_features\n",
    "            model_ft.fc8 = nn.Linear(num_ftrs, self.num_classes)\n",
    "            input_size = 224\n",
    "        \n",
    "        self.model_ft = model_ft\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        return model_ft, input_size \n",
    "    \n",
    "    def get_batches(self,dataloader, batch_size):\n",
    "        random.shuffle(dataloader)\n",
    "        batches = []\n",
    "\n",
    "        num_batches = int(len(dataloader)/batch_size)\n",
    "        start_idx = 0\n",
    "        for i in range(num_batches):\n",
    "            batches.append(dataloader[start_idx:start_idx+batch_size])\n",
    "            start_idx += batch_size\n",
    "\n",
    "        batches.append(dataloader[start_idx:])\n",
    "        return batches\n",
    "        \n",
    "    def train_model(self,model,device,dataloaders, dataset_sizes, criterion, optimizer,num_epochs=25): \n",
    "        \"\"\"\n",
    "        return: model + val_acc_history\n",
    "        \n",
    "        \"\"\"\n",
    "        #start time \n",
    "        since = time.time()\n",
    "\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        val_acc_history = []\n",
    "        training_diagnostics = {\"Loss\": []}\n",
    "        validation_diagnostics = {\"Loss\": [], \"Accuracy\": []}\n",
    "        \n",
    "        #send model to gpu \n",
    "        model = model.to(device)\n",
    "\n",
    "        #for each epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "            #train and evaluate at current model at each epoch\n",
    "            for phase in [\"train\",\"val\"]:\n",
    "                if phase == \"train\":\n",
    "                    model.train() #set model to training mode\n",
    "                else:\n",
    "                    model.eval() #set model to evaluate mode\n",
    "\n",
    "                #keep track of loss \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                \n",
    "                #get batches \n",
    "                batches = self.get_batches(dataloaders[phase],32)\n",
    "                for batch in batches:\n",
    "                    # load batch \n",
    "                    batch_tensors = [torch.load(path) for path in batch]\n",
    "                    inputs = torch.stack([x[0] for x in batch_tensors])\n",
    "                    labels = torch.LongTensor([x[1] for x in batch_tensors])\n",
    "                    \n",
    "                    # send to gpu\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    #zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    #forward prop \n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        outputs = model(inputs) #predict\n",
    "                        if self.model_name == 'vggface2':\n",
    "                            outputs = outputs[0]\n",
    "                        _,preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        #backward + optimize only if training phase \n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    #stats \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "#                 if phase == \"train\":\n",
    "#                     scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss/dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() /dataset_sizes[phase]\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "                # Store Losses and Accuracy in Tensorboard\n",
    "                if phase == \"train\":\n",
    "                    training_diagnostics[\"Loss\"].append((epoch,epoch_loss))\n",
    "                else:\n",
    "                    validation_diagnostics[\"Loss\"].append((epoch, epoch_loss))\n",
    "                    validation_diagnostics[\"Accuracy\"].append((epoch,epoch_acc.item()))\n",
    "                    \n",
    "                if phase == \"val\" and epoch_acc > best_acc: \n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model.state_dict())\n",
    "                if phase == \"val\":\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        model.load_state_dict(best_model)\n",
    "        \n",
    "        return model, val_acc_history, training_diagnostics, validation_diagnostics\n",
    "    \n",
    "    def evaluate_model(self, model_ft, dataloaders, device):\n",
    "        correct = 0 \n",
    "        total = 0 \n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        if self.progressive: \n",
    "            batches = self.get_batches(dataloaders[3][\"test\"],32)\n",
    "        else:\n",
    "            batches = self.get_batches(dataloaders[\"test\"],32)\n",
    "        with torch.no_grad():\n",
    "            for batch in batches:\n",
    "                # load batch \n",
    "                batch_tensors = [torch.load(path) for path in batch]\n",
    "                inputs = torch.stack([x[0] for x in batch_tensors])\n",
    "                labels = torch.LongTensor([x[1] for x in batch_tensors])\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model_ft(inputs)\n",
    "                if self.model_name == \"vggface2\":\n",
    "                    outputs = outputs[0]\n",
    "                _,predicted = torch.max(outputs.data,1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "                all_preds.append(predicted.cpu().detach().numpy())\n",
    "                all_labels.append(labels.cpu().detach().numpy())\n",
    "                \n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "                \n",
    "        f1 = f1_score(all_labels, all_preds, average='macro' )\n",
    "        precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        #Accuracy \n",
    "        accuracy = correct/total * 100\n",
    "        \n",
    "        # Unweighted Average F1 Score \n",
    "        \n",
    "        return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "We ran all our experiments changing the following parameters according to our needs\n",
    "- model_name\n",
    "- num_classes\n",
    "- feature_extract\n",
    "- use_pretrained\n",
    "- learning_rate\n",
    "- momentum\n",
    "- dataloaders\n",
    "- dataset_sizes\n",
    "- progressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment \n",
    "new_experiment = FacialExpressionModel(\"resnet\", 7,dataloaders,dataset_sizes, lr=0.001, momentum=0.9,\n",
    "                 feature_extract=False, use_pretrained=False,progressive=False) #add in epochs as well\n",
    "model_ft, val_hist,train_d, val_d, accuracy, f1 = new_experiment.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results\n",
    "We save the train and validation accuracies and loss in json files to compare and contrast various experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Diagnostics\n",
    "path = \"./runs/resnet\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "train_json = json.dumps(train_d)\n",
    "val_json = json.dumps(val_d)\n",
    "\n",
    "train_file = open(path+\"/train.json\",\"w\")\n",
    "train_file.write(train_json)\n",
    "train_file.close()\n",
    "\n",
    "val_file = open(path+\"/val.json\",\"w\")\n",
    "val_file.write(val_json)\n",
    "val_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
